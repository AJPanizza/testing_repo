{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e67be3a2-18e1-4251-8e1c-f5ae6579b5ed",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuration and Setup"
    }
   },
   "outputs": [],
   "source": [
    "# Auto Loader - Triggered execution for batch processing\n",
    "# Configuration using dbutils widgets for flexibility\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "# Create dbutils widgets for configuration\n",
    "dbutils.widgets.text(\"catalog\", \"samples\", \"Catalog Name\")\n",
    "dbutils.widgets.text(\"schema\", \"default\", \"Schema Name\")\n",
    "dbutils.widgets.text(\"volume\", \"autoloader_volume\", \"Volume Name\")\n",
    "\n",
    "# Get configuration values\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "volume = dbutils.widgets.get(\"volume\")\n",
    "\n",
    "spark.sql(f\"USE CATALOG {catalog}\")\n",
    "spark.sql(f\"USE SCHEMA {schema}\")\n",
    "\n",
    "# Volume paths\n",
    "volume_path = f\"/Volumes/{catalog}/{schema}/{volume}\"\n",
    "checkpoint_base_path = f\"/Volumes/{catalog}/{schema}/checkpoints\"\n",
    "schema_base_path = f\"/Volumes/{catalog}/{schema}/{volume}/schemas\"\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"Catalog: {catalog}\")\n",
    "print(f\"Schema: {schema}\")\n",
    "print(f\"Volume: {volume}\")\n",
    "print(f\"Volume Path: {volume_path}\")\n",
    "\n",
    "add_table = \"bronze_add\"\n",
    "contact_table = \"contact_silver\"\n",
    "contact_segment_table = \"contact_segment_table\"\n",
    "remove_table = \"bronze_remove\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9287383c-215c-4065-93fe-3e238c8c5179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "add_files_schema = StructType([\n",
    "    StructField(\"Name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), False),\n",
    "    StructField(\"DOB\", DateType(), True),\n",
    "    StructField(\"occupation\", StringType(), True),\n",
    "    StructField(\"phone_number\", StringType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# Create add data table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {add_table} (\n",
    "  Name STRING,\n",
    "  email STRING,\n",
    "  DOB DATE,\n",
    "  occupation STRING,\n",
    "  phone_number STRING,\n",
    "  source_file STRING,\n",
    "  ingestion_timestamp TIMESTAMP,\n",
    "  file_modification_time TIMESTAMP\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b671f26-646e-4742-834e-5d7fcb140739",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Process ADD Files (Triggered)"
    }
   },
   "outputs": [],
   "source": [
    "add_source_path = f\"{volume_path}/*/add/*.csv\"\n",
    "add_checkpoint_path = f\"{checkpoint_base_path}/add_stream\"\n",
    "batch_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(f\"Processing ADD files from: {add_source_path}\")\n",
    "print(f\"Batch ID: {batch_id}\")\n",
    "\n",
    "add_df = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .schema(add_files_schema)\n",
    "    .option(\"cloudFiles.maxFilesPerTrigger\", \"1000\")\n",
    "    \n",
    "    # CSV-specific options\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    \n",
    "    .load(add_source_path)\n",
    "    .withColumn(\"source_file\", col(\"_metadata.file_path\"))\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "    .withColumn(\"file_modification_time\", col(\"_metadata.file_modification_time\"))\n",
    ")\n",
    "\n",
    "# Process with triggered execution (availableNow)\n",
    "add_query = (add_df\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", add_checkpoint_path)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(availableNow=True) \n",
    "    .toTable(add_table)\n",
    ")\n",
    "\n",
    "# Wait for completion\n",
    "add_query.awaitTermination()\n",
    "print(\"âœ“ ADD files processed successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c185de9-0d93-417e-8a33-920ca619c8d9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SCD 1 contact table"
    }
   },
   "outputs": [],
   "source": [
    "# create the scd silver table\n",
    "from pyspark.sql import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "contact_checkpoint_path = f\"{checkpoint_base_path}/contact\"\n",
    "# Create historical contact data\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {contact_table} (\n",
    "  Name STRING,\n",
    "  email STRING,\n",
    "  DOB DATE,\n",
    "  occupation STRING,\n",
    "  phone_number STRING\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "target_columns = [\"Name\", \"DOB\", \"occupation\", \"phone_number\"] \n",
    "\n",
    "def upsert_to_contacts(microbatch_df, _):\n",
    "    # pick latest per id inside the microbatch\n",
    "    w = Window.partitionBy(\"email\").orderBy(col(\"file_modification_time\").desc())\n",
    "    latest = (microbatch_df\n",
    "              .withColumn(\"rn\", row_number().over(w))\n",
    "              .filter(\"rn = 1\")\n",
    "              .drop(\"rn\"))\n",
    "\n",
    "    tgt = DeltaTable.forName(spark, \"contact_silver\")\n",
    "    \n",
    "    # TODO - check that the update is newer than the existing value \n",
    "    # Update condition: only update if source has non-null values that are different from target\n",
    "    update_conditions = []\n",
    "    for c in target_columns:\n",
    "        update_conditions.append(f\"(s.{c} IS NOT NULL AND (t.{c} IS NULL OR t.{c} != s.{c}))\")\n",
    "    update_cond = \" OR \".join(update_conditions) if update_conditions else \"false\"\n",
    "    \n",
    "    # Update set: only update columns where source is not null\n",
    "    update_set = {}\n",
    "    for c in target_columns:\n",
    "        update_set[c] = when(col(f\"s.{c}\").isNotNull(), col(f\"s.{c}\")).otherwise(col(f\"t.{c}\"))\n",
    "    \n",
    "    # Insert values: use source values, including email as the key\n",
    "    insert_vals = {c: col(f\"s.{c}\") for c in [\"email\"] + target_columns}\n",
    "    \n",
    "    (tgt.alias(\"t\")\n",
    "        .merge(latest.alias(\"s\"), \"t.email = s.email\")\n",
    "        .whenMatchedUpdate(condition=update_cond, set=update_set)\n",
    "        .whenNotMatchedInsert(values=insert_vals)\n",
    "        .execute())\n",
    "\n",
    "bronze_df = (spark.readStream\n",
    "  .table(add_table)\n",
    ")\n",
    "\n",
    "writer = (bronze_df.writeStream\n",
    "  .foreachBatch(upsert_to_contacts)\n",
    "  .option(\"checkpointLocation\", contact_checkpoint_path)\n",
    "  .trigger(availableNow=True)\n",
    ")\n",
    "\n",
    "q = writer.start()\n",
    "q.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "396861cc-bc60-4bd9-b081-bac4b7f9a532",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Contact/Segment table add"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, element_at, col, row_number\n",
    "from pyspark.sql import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "contact_segment_checkpoint = f\"{checkpoint_base_path}/contact_segment\"\n",
    "# Create historical contact data\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {contact_segment_table} (\n",
    "  email STRING,\n",
    "  segment STRING,\n",
    "  file_modification_time TIMESTAMP,\n",
    "  is_active BOOLEAN\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "bronze_df = (spark.readStream\n",
    "  .table(add_table)\n",
    ")\n",
    "\n",
    "bronze_df = bronze_df.select(\n",
    "        \"email\",\n",
    "        \"file_modification_time\",\n",
    "        element_at(split(\"source_file\", \"/\"), -3).alias(\"segment\")\n",
    "    )\n",
    "\n",
    "target_columns = [\"email\", \"segment\"] \n",
    "\n",
    "def upsert_to_contact_segment(microbatch_df, _):\n",
    "    w = Window.partitionBy(\"email\", \"segment\").orderBy(col(\"file_modification_time\").desc())\n",
    "    latest = (microbatch_df\n",
    "              .withColumn(\"rn\", row_number().over(w))\n",
    "              .filter(\"rn = 1\")\n",
    "              .drop(\"rn\"))\n",
    "\n",
    "    tgt = DeltaTable.forName(spark, contact_segment_table)\n",
    "    \n",
    "    # Insert values: insert s.email and s.segment\n",
    "    insert_vals = {'email': col('s.email'), 'segment': col('s.segment'), \"file_modification_time\": col('s.file_modification_time'), \"is_active\": lit(True)}\n",
    "\n",
    "    (tgt.alias(\"t\")\n",
    "        .merge(latest.alias(\"s\"), \"t.email = s.email AND t.segment = s.segment AND t.file_modification_time < s.file_modification_time\")\n",
    "        .whenNotMatchedInsert(values=insert_vals)\n",
    "        .execute())\n",
    "\n",
    "writer = (bronze_df.writeStream\n",
    "  .foreachBatch(upsert_to_contact_segment)\n",
    "  .option(\"checkpointLocation\", contact_segment_checkpoint)\n",
    "  .trigger(availableNow=True)\n",
    ")\n",
    "\n",
    "q = writer.start()\n",
    "q.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "573dba2a-aae8-40c1-a6db-dde9a6643fb3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Process REMOVE files (Triggered)"
    }
   },
   "outputs": [],
   "source": [
    "# Create remove data table\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {remove_table} (\n",
    "  email STRING,\n",
    "  source_file STRING,\n",
    "  ingestion_timestamp TIMESTAMP,\n",
    "  file_modification_time TIMESTAMP\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "remove_source_path = f\"{volume_path}/*/remove/*.csv\"\n",
    "remove_checkpoint_path = f\"{checkpoint_base_path}/remove_stream\"\n",
    "remove_schema_path = schema_base_path+\"/remove_schema\"\n",
    "batch_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(f\"Processing REMOVE files from: {remove_source_path}\")\n",
    "print(f\"Batch ID: {batch_id}\")\n",
    "\n",
    "\n",
    "remove_df = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.schemaLocation\", remove_schema_path)\n",
    "    \n",
    "    # CSV-specific options\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "\n",
    "    .load(remove_source_path)\n",
    "    .select(\"email\")\n",
    "    .withColumn(\"source_file\", col(\"_metadata.file_path\"))\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "    .withColumn(\"file_modification_time\", col(\"_metadata.file_modification_time\"))\n",
    ")\n",
    "\n",
    "# Process with triggered execution (availableNow)\n",
    "remove_query = (remove_df\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", remove_checkpoint_path)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(availableNow=True) \n",
    "    .toTable(remove_table)\n",
    ")\n",
    "\n",
    "# Wait for completion\n",
    "remove_query.awaitTermination()\n",
    "print(\"âœ“ REMOVE files processed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc799004-58c4-473c-8ff7-5e92c9ce6ce7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Contacts/Segments table remove"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, element_at, col, row_number\n",
    "from pyspark.sql import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "contact_segment_table = \"contact_segment_table\"\n",
    "contact_segment_rm_checkpoint = f\"{checkpoint_base_path}/contact_segment_remove\"\n",
    "\n",
    "bronze_remove_df = (spark.readStream\n",
    "  .table(remove_table)\n",
    ")\n",
    "\n",
    "bronze_remove_df = bronze_remove_df.select(\n",
    "        \"email\",\n",
    "        \"file_modification_time\",\n",
    "        element_at(split(\"source_file\", \"/\"), -3).alias(\"segment\")\n",
    "    )\n",
    "\n",
    "target_columns = [\"email\", \"segment\"] \n",
    "\n",
    "def remove_from_contact_segment(microbatch_df, _):\n",
    "    w = Window.partitionBy(\"email\", \"segment\").orderBy(col(\"file_modification_time\").desc())\n",
    "    latest = (microbatch_df\n",
    "              .withColumn(\"rn\", row_number().over(w))\n",
    "              .filter(\"rn = 1\")\n",
    "              .drop(\"rn\"))\n",
    "\n",
    "    tgt = DeltaTable.forName(spark, contact_segment_table)\n",
    "    \n",
    "    # Insert soft delete values: delete s.email and s.segment\n",
    "    delete_vals = {'email': col('s.email'), 'segment': col('s.segment'), \"file_modification_time\": col('s.file_modification_time'), \"is_active\": lit(False)}\n",
    "    update_cond = \"t.file_modification_time < s.file_modification_time\"\n",
    "\n",
    "    (tgt.alias(\"t\")\n",
    "        .merge(latest.alias(\"s\"), \"t.email = s.email AND t.segment = s.segment\")\n",
    "        .whenMatchedUpdate(condition=update_cond, set=delete_vals)\n",
    "        .execute())\n",
    "\n",
    "writer = (bronze_remove_df.writeStream\n",
    "  .foreachBatch(remove_from_contact_segment)\n",
    "  .option(\"checkpointLocation\", contact_segment_rm_checkpoint)\n",
    "  .trigger(availableNow=True)\n",
    ")\n",
    "\n",
    "q = writer.start()\n",
    "q.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43b40f9f-4db1-4c4d-9925-b93955babcd4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Utility Functions and Manual Trigger"
    }
   },
   "outputs": [],
   "source": [
    "# Utility functions for managing Auto Loader\n",
    "\n",
    "def trigger_autoloader_processing():\n",
    "    \"\"\"\n",
    "    Function to trigger Auto Loader processing for both ADD and REMOVE files.\n",
    "    This can be called manually or scheduled as needed.\n",
    "    \"\"\"\n",
    "    batch_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    print(f\"ðŸš€ Starting Auto Loader processing - Batch ID: {batch_id}\")\n",
    "    \n",
    "    try:\n",
    "        # This would contain the processing logic from cells 3-5\n",
    "        print(\"\\n1ï¸âƒ£ Processing ADD files...\")\n",
    "        print(\"\\n2ï¸âƒ£ Processing REMOVE files...\")\n",
    "        print(\"\\n3ï¸âƒ£ Updating incremental tables...\")\n",
    "        \n",
    "        print(f\"\\nâœ… Batch {batch_id} completed successfully!\")\n",
    "        return batch_id\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error in batch {batch_id}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def reset_checkpoints():\n",
    "    \"\"\"\n",
    "    Reset Auto Loader checkpoints to reprocess all files.\n",
    "    Use with caution - this will cause all files to be reprocessed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dbutils.fs.rm(f\"{checkpoint_base_path}\", True)\n",
    "        print(\"âœ… Checkpoints reset successfully\")\n",
    "        print(\"âš ï¸  Next run will reprocess ALL files\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error resetting checkpoints: {str(e)}\")\n",
    "\n",
    "def view_checkpoint_status():\n",
    "    \"\"\"\n",
    "    View the current checkpoint status and processed files.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if checkpoint directories exist\n",
    "        try:\n",
    "            add_checkpoint_files = dbutils.fs.ls(f\"{checkpoint_base_path}/add_stream\")\n",
    "            add_checkpoint_count = len(add_checkpoint_files)\n",
    "        except:\n",
    "            add_checkpoint_count = 0\n",
    "            \n",
    "        try:\n",
    "            remove_checkpoint_files = dbutils.fs.ls(f\"{checkpoint_base_path}/remove_stream\")\n",
    "            remove_checkpoint_count = len(remove_checkpoint_files)\n",
    "        except:\n",
    "            remove_checkpoint_count = 0\n",
    "        \n",
    "        print(f\"ðŸ“‚ ADD Checkpoint Status: {add_checkpoint_count} files\")\n",
    "        print(f\"ðŸ“‚ REMOVE Checkpoint Status: {remove_checkpoint_count} files\")\n",
    "        \n",
    "        # Show latest processed files\n",
    "        try:\n",
    "            latest_processed = spark.sql(f\"\"\"\n",
    "                SELECT file_type, COUNT(*) as file_count, MAX(processed_timestamp) as last_processed\n",
    "                FROM {processed_files_table}\n",
    "                GROUP BY file_type\n",
    "            \"\"\")\n",
    "            \n",
    "            print(\"\\nðŸ“ˆ Processing Summary:\")\n",
    "            latest_processed.display()\n",
    "        except:\n",
    "            print(\"\\nðŸ“ˆ No processing history available yet\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error checking status: {str(e)}\")\n",
    "\n",
    "# Example usage functions\n",
    "print(\"ðŸ› ï¸  Utility functions available:\")\n",
    "print(\"- trigger_autoloader_processing(): Run processing manually\")\n",
    "print(\"- view_checkpoint_status(): Check processing status\")\n",
    "print(\"- reset_checkpoints(): Reset to reprocess all files (use with caution)\")\n",
    "print(\"\\nðŸ“ Usage Examples:\")\n",
    "print(\"# batch_id = trigger_autoloader_processing()\")\n",
    "print(\"# view_checkpoint_status()\")\n",
    "print(\"# reset_checkpoints()  # Use with caution!\")\n",
    "print(\"\\nðŸ“… Scheduling Options:\")\n",
    "print(\"- Run this notebook on a schedule using Databricks Jobs\")\n",
    "print(\"- Call trigger_autoloader_processing() from another notebook\")\n",
    "print(\"- Use Databricks Workflows to orchestrate processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae8f43a6-688e-4e5b-a8ed-dd7dfc55fb91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "view_checkpoint_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7878b494-537a-41df-a121-f9b6b53bb2d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"DROP TABLE IF EXISTS {add_table}\"))\n",
    "display(spark.sql(f\"DROP TABLE IF EXISTS {contact_table}\"))\n",
    "display(spark.sql(f\"DROP TABLE IF EXISTS {contact_segment_table}\"))\n",
    "display(spark.sql(f\"DROP TABLE IF EXISTS {remove_table}\"))\n",
    "display(spark.sql(f\"DROP TABLE IF EXISTS main_table\"))\n",
    "display(spark.sql(f\"DROP TABLE IF EXISTS segments\")) \n",
    "dbutils.fs.rm(checkpoint_base_path, True)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8820935236977705,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "AutoLoader",
   "widgets": {
    "catalog": {
     "currentValue": "agustin_training_catalog",
     "nuid": "22420599-dac5-498e-8368-2a893c917d13",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "samples",
      "label": "Catalog Name",
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "samples",
      "label": "Catalog Name",
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "schema": {
     "currentValue": "dev_agustin_panizza_sfmc",
     "nuid": "4d6b4684-1d87-4a6c-9272-dfe63b0da2a7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": "Schema Name",
      "name": "schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default",
      "label": "Schema Name",
      "name": "schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "volume": {
     "currentValue": "files",
     "nuid": "3b3e45ec-af43-48e5-95d7-a463ad1737d0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "autoloader_volume",
      "label": "Volume Name",
      "name": "volume",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "autoloader_volume",
      "label": "Volume Name",
      "name": "volume",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
