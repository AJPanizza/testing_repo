{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9839bb1-9556-46ef-ba47-e1c4c0a4080c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate Mock CSV Files with Sample Data"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Mock data lists\n",
    "first_names = ['John', 'Jane', 'Michael', 'Sarah', 'David', 'Emily', 'Robert', 'Lisa', 'James', 'Maria', \n",
    "               'William', 'Jennifer', 'Richard', 'Patricia', 'Charles', 'Linda', 'Joseph', 'Elizabeth', \n",
    "               'Thomas', 'Barbara', 'Christopher', 'Susan', 'Daniel', 'Jessica', 'Matthew', 'Karen',\n",
    "               'Mark', 'Nancy', 'Paul', 'Betty', 'Donald', 'Helen', 'George', 'Sandra', 'Kenneth', 'Donna',\n",
    "               'Steven', 'Carol', 'Edward', 'Ruth', 'Brian', 'Sharon', 'Ronald', 'Michelle', 'Anthony', 'Laura',\n",
    "               'Kevin', 'Sarah', 'Jason', 'Kimberly', 'Jeffrey', 'Deborah', 'Ryan', 'Dorothy', 'Jacob', 'Amy',\n",
    "               'Gary', 'Angela', 'Nicholas', 'Brenda', 'Eric', 'Emma', 'Jonathan', 'Olivia', 'Stephen', 'Cynthia',\n",
    "               'Larry', 'Marie', 'Justin', 'Janet', 'Scott', 'Catherine', 'Brandon', 'Frances', 'Benjamin', 'Christine']\n",
    "\n",
    "last_names = ['Smith', 'Johnson', 'Williams', 'Brown', 'Jones', 'Garcia', 'Miller', 'Davis', 'Rodriguez', \n",
    "              'Martinez', 'Hernandez', 'Lopez', 'Gonzalez', 'Wilson', 'Anderson', 'Thomas', 'Taylor', \n",
    "              'Moore', 'Jackson', 'Martin', 'Lee', 'Perez', 'Thompson', 'White', 'Harris', 'Sanchez',\n",
    "              'Clark', 'Ramirez', 'Lewis', 'Robinson', 'Walker', 'Young', 'Allen', 'King', 'Wright',\n",
    "              'Scott', 'Torres', 'Nguyen', 'Hill', 'Flores', 'Green', 'Adams', 'Nelson', 'Baker',\n",
    "              'Hall', 'Rivera', 'Campbell', 'Mitchell', 'Carter', 'Roberts', 'Gomez', 'Phillips',\n",
    "              'Evans', 'Turner', 'Diaz', 'Parker', 'Cruz', 'Edwards', 'Collins', 'Reyes', 'Stewart',\n",
    "              'Morris', 'Morales', 'Murphy', 'Cook', 'Rogers', 'Gutierrez', 'Ortiz', 'Morgan', 'Cooper']\n",
    "\n",
    "occupations = ['Software Engineer', 'Teacher', 'Doctor', 'Nurse', 'Accountant', 'Marketing Manager', \n",
    "               'Sales Representative', 'Data Analyst', 'Project Manager', 'Graphic Designer', \n",
    "               'Lawyer', 'Consultant', 'Engineer', 'Administrator', 'Researcher', 'Writer', \n",
    "               'Chef', 'Mechanic', 'Electrician', 'Pharmacist', 'Architect', 'Therapist',\n",
    "               'Financial Advisor', 'HR Manager', 'Operations Manager', 'Business Analyst',\n",
    "               'Product Manager', 'UX Designer', 'DevOps Engineer', 'Quality Assurance',\n",
    "               'Customer Service Rep', 'Real Estate Agent', 'Insurance Agent', 'Banker']\n",
    "\n",
    "def generate_email(first_name, last_name):\n",
    "    \"\"\"Generate a mock email address\"\"\"\n",
    "    domains = ['gmail.com', 'yahoo.com', 'hotmail.com', 'outlook.com', 'company.com', 'email.com']\n",
    "    return f\"{first_name.lower()}.{last_name.lower()}@{random.choice(domains)}\"\n",
    "\n",
    "def generate_dob():\n",
    "    \"\"\"Generate a random date of birth between 1950 and 2000\"\"\"\n",
    "    start_date = datetime(1950, 1, 1)\n",
    "    end_date = datetime(2000, 12, 31)\n",
    "    time_between = end_date - start_date\n",
    "    days_between = time_between.days\n",
    "    random_days = random.randrange(days_between)\n",
    "    return (start_date + timedelta(days=random_days)).strftime('%Y-%m-%d')\n",
    "\n",
    "def generate_phone():\n",
    "    \"\"\"Generate a mock phone number\"\"\"\n",
    "    # Sometimes return None to simulate missing data\n",
    "    if random.random() < 0.1:  # 10% chance of missing phone\n",
    "        return None\n",
    "    return f\"+1-{random.randint(200,999)}-{random.randint(200,999)}-{random.randint(1000,9999)}\"\n",
    "\n",
    "def generate_person():\n",
    "    \"\"\"Generate a single person's data\"\"\"\n",
    "    first_name = random.choice(first_names)\n",
    "    last_name = random.choice(last_names)\n",
    "    \n",
    "    # Sometimes make name None to simulate missing data\n",
    "    name = f\"{first_name} {last_name}\" if random.random() > 0.05 else None  # 5% chance of missing name\n",
    "    \n",
    "    return {\n",
    "        'Name': name,\n",
    "        'email': generate_email(first_name, last_name),  # Email is never empty as required\n",
    "        'DOB': generate_dob() if random.random() > 0.08 else None,  # 8% chance of missing DOB\n",
    "        'occupation': random.choice(occupations) if random.random() > 0.12 else None,  # 12% chance of missing occupation\n",
    "        'phone_number': generate_phone()\n",
    "    }\n",
    "\n",
    "# Generate a pool of 200 people that can be reused across CSVs\n",
    "people_pool = [generate_person() for _ in range(200)]\n",
    "\n",
    "print(f\"Generated {len(people_pool)} unique people for the pool\")\n",
    "print(\"\\nSample person:\")\n",
    "print(people_pool[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fa28ce1-058d-4cad-841d-24dc06ca93e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create CSV Files with Overlapping Data"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration for catalog storage\n",
    "catalog = \"agustin_training_catalog\"\n",
    "schema = \"dev_agustin_panizza_sfmc\"\n",
    "table_name = \"mock_files\"\n",
    "full_table_name = f\"{catalog}.{schema}.{table_name}\"\n",
    "\n",
    "# Create directory for temporary CSV files\n",
    "csv_dir = f\"/Volumes/{catalog}/{schema}/mock_files\"\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "# Create 10 different CSV files with some overlapping people\n",
    "csv_files_data = []\n",
    "all_file_data = []  # To store all data for the catalog table\n",
    "\n",
    "for i in range(1, 11):  # Create 10 files\n",
    "    # Each CSV will have exactly 30 records\n",
    "    num_records = 30\n",
    "    \n",
    "    # Mix of people from the pool (to create overlaps) and new people\n",
    "    csv_data = []\n",
    "    \n",
    "    # Add some people from the pool (for overlap)\n",
    "    pool_people_count = random.randint(15, 25)  # 15-25 people from pool\n",
    "    selected_pool_people = random.sample(people_pool, pool_people_count)\n",
    "    csv_data.extend(selected_pool_people)\n",
    "    \n",
    "    # Add some new people specific to this CSV\n",
    "    remaining_count = num_records - len(csv_data)\n",
    "    for _ in range(remaining_count):\n",
    "        csv_data.append(generate_person())\n",
    "    \n",
    "    # Shuffle the data\n",
    "    random.shuffle(csv_data)\n",
    "    \n",
    "    # Create DataFrame and save to temporary CSV\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    csv_filename = f\"{csv_dir}/people_data_{i:02d}.csv\"\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "    \n",
    "    # Add file metadata for catalog storage\n",
    "    df['file_name'] = f\"people_data_{i:02d}.csv\"\n",
    "    df['file_id'] = i\n",
    "    df['created_timestamp'] = datetime.now()\n",
    "    \n",
    "    # Add to all_file_data for catalog storage\n",
    "    all_file_data.append(df)\n",
    "    \n",
    "    csv_files_data.append({\n",
    "        'filename': csv_filename,\n",
    "        'file_id': i,\n",
    "        'records': len(df) - 3,  # Subtract metadata columns\n",
    "        'non_null_emails': df['email'].notna().sum(),\n",
    "        'null_names': df['Name'].isna().sum(),\n",
    "        'null_dobs': df['DOB'].isna().sum(),\n",
    "        'null_occupations': df['occupation'].isna().sum(),\n",
    "        'null_phones': df['phone_number'].isna().sum()\n",
    "    })\n",
    "    \n",
    "    print(f\"Created {csv_filename} with {num_records} records\")\n",
    "\n",
    "# Combine all DataFrames for catalog storage\n",
    "combined_df = pd.concat(all_file_data, ignore_index=True)\n",
    "spark_df = spark.createDataFrame(combined_df)\n",
    "\n",
    "# Store in catalog table\n",
    "print(f\"\\nüìä Storing all data in catalog table: {full_table_name}\")\n",
    "try:\n",
    "    # Drop table if exists and create new one\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {full_table_name}\")\n",
    "    spark_df.write.mode(\"overwrite\").saveAsTable(full_table_name)\n",
    "    print(f\"‚úÖ Successfully stored {len(combined_df)} records in {full_table_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error storing in catalog: {e}\")\n",
    "    print(\"Will continue with local CSV files only\")\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(csv_files_data)} CSV files in {csv_dir}\")\n",
    "print(f\"üìä Total records across all files: {sum(info['records'] for info in csv_files_data)}\")\n",
    "print(\"\\nüìã Summary of created files:\")\n",
    "for file_info in csv_files_data:\n",
    "    print(f\"\\nFile {file_info['file_id']:2d}: {file_info['filename']}\")\n",
    "    print(f\"  - Total records: {file_info['records']}\")\n",
    "    print(f\"  - Non-null emails: {file_info['non_null_emails']} (should equal total records)\")\n",
    "    print(f\"  - Null names: {file_info['null_names']}\")\n",
    "    print(f\"  - Null DOBs: {file_info['null_dobs']}\")\n",
    "    print(f\"  - Null occupations: {file_info['null_occupations']}\")\n",
    "    print(f\"  - Null phone numbers: {file_info['null_phones']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d488e75-fe63-48f1-9ca1-99b41187c83f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display Sample Data from Each CSV"
    }
   },
   "outputs": [],
   "source": [
    "# Display sample data from the catalog table and individual files\n",
    "print(\"üìã Sample data from catalog table and individual CSV files:\\n\")\n",
    "\n",
    "# Show catalog table summary\n",
    "try:\n",
    "    print(f\"=== Catalog Table: {full_table_name} ===\")\n",
    "    catalog_df = spark.table(full_table_name)\n",
    "    print(f\"Total records in catalog: {catalog_df.count()}\")\n",
    "    print(f\"Unique files: {catalog_df.select('file_id').distinct().count()}\")\n",
    "    \n",
    "    print(\"\\nSample records from catalog table:\")\n",
    "    catalog_df.select(\"Name\", \"email\", \"DOB\", \"occupation\", \"phone_number\", \"file_name\", \"file_id\").limit(10).display()\n",
    "    \n",
    "    print(\"\\nRecords per file in catalog:\")\n",
    "    catalog_df.groupBy(\"file_id\", \"file_name\").count().orderBy(\"file_id\").display()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error reading from catalog table: {e}\")\n",
    "    print(\"Showing local CSV files instead...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"\\nüìÅ Individual CSV Files Sample Data:\\n\")\n",
    "\n",
    "# Display sample from first 3 CSV files\n",
    "for i in range(1, min(4, len(csv_files_data) + 1)):\n",
    "    file_info = csv_files_data[i-1]\n",
    "    print(f\"=== CSV File {file_info['file_id']}: {file_info['filename']} ===\")\n",
    "    \n",
    "    # Read and display first 5 rows\n",
    "    df = pd.read_csv(file_info['filename'])\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"\\nFirst 5 rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    # Show data quality summary\n",
    "    print(\"\\nData Quality Summary:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "if len(csv_files_data) > 3:\n",
    "    print(f\"... and {len(csv_files_data) - 3} more files with similar structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80c5a763-083a-4330-8197-a5c465b4799a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Verify Email Uniqueness and Overlaps"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze overlaps between CSV files by checking email addresses\n",
    "all_emails = set()\n",
    "file_emails = {}\n",
    "overlapping_emails = set()\n",
    "\n",
    "print(\"üîç Analyzing email overlaps between 10 CSV files:\\n\")\n",
    "\n",
    "for i, file_info in enumerate(csv_files_data, 1):\n",
    "    df = pd.read_csv(file_info['filename'])\n",
    "    file_emails[f\"CSV_{i:02d}\"] = set(df['email'].dropna())\n",
    "    \n",
    "    # Check for overlaps with previously processed files\n",
    "    current_overlaps = all_emails.intersection(file_emails[f\"CSV_{i:02d}\"])\n",
    "    if current_overlaps:\n",
    "        overlapping_emails.update(current_overlaps)\n",
    "        print(f\"CSV {i:2d} has {len(current_overlaps):2d} overlapping emails with previous files\")\n",
    "        if len(current_overlaps) <= 5:\n",
    "            print(f\"       Overlapping emails: {list(current_overlaps)}\")\n",
    "        else:\n",
    "            print(f\"       Sample overlapping emails: {list(current_overlaps)[:5]}...\")\n",
    "    else:\n",
    "        print(f\"CSV {i:2d} has  0 overlapping emails with previous files\")\n",
    "    \n",
    "    all_emails.update(file_emails[f\"CSV_{i:02d}\"])\n",
    "    print(f\"       CSV {i:2d} contains {len(file_emails[f'CSV_{i:02d}']):2d} unique emails\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nüìä Overall Statistics:\")\n",
    "print(f\"Total unique emails across all files: {len(all_emails)}\")\n",
    "print(f\"Total overlapping emails: {len(overlapping_emails)}\")\n",
    "print(f\"Overlap percentage: {len(overlapping_emails)/len(all_emails)*100:.1f}%\")\n",
    "print(f\"Average emails per file: {sum(len(emails) for emails in file_emails.values()) / len(file_emails):.1f}\")\n",
    "\n",
    "# Show pairwise overlaps (only show significant ones to avoid clutter)\n",
    "print(\"\\nüîó Significant pairwise overlaps between files (>= 3 shared emails):\")\n",
    "overlap_count = 0\n",
    "for i in range(1, len(csv_files_data) + 1):\n",
    "    for j in range(i + 1, len(csv_files_data) + 1):\n",
    "        overlap = file_emails[f\"CSV_{i:02d}\"].intersection(file_emails[f\"CSV_{j:02d}\"])\n",
    "        if len(overlap) >= 3:  # Only show significant overlaps\n",
    "            print(f\"CSV {i:2d} ‚Üî CSV {j:2d}: {len(overlap):2d} shared emails\")\n",
    "            overlap_count += 1\n",
    "\n",
    "if overlap_count == 0:\n",
    "    print(\"No significant pairwise overlaps found (all overlaps < 3 emails)\")\n",
    "\n",
    "# Show distribution of overlap sizes\n",
    "print(\"\\nüìà Overlap Distribution:\")\n",
    "overlap_sizes = []\n",
    "for i in range(1, len(csv_files_data) + 1):\n",
    "    for j in range(i + 1, len(csv_files_data) + 1):\n",
    "        overlap_size = len(file_emails[f\"CSV_{i:02d}\"].intersection(file_emails[f\"CSV_{j:02d}\"]))\n",
    "        overlap_sizes.append(overlap_size)\n",
    "\n",
    "if overlap_sizes:\n",
    "    print(f\"Min overlap: {min(overlap_sizes)} emails\")\n",
    "    print(f\"Max overlap: {max(overlap_sizes)} emails\")\n",
    "    print(f\"Avg overlap: {sum(overlap_sizes)/len(overlap_sizes):.1f} emails\")\n",
    "    print(f\"Total pairwise comparisons: {len(overlap_sizes)}\")\n",
    "\n",
    "# Verify catalog table data if available\n",
    "try:\n",
    "    print(\"\\nüìä Catalog Table Verification:\")\n",
    "    catalog_df = spark.table(full_table_name)\n",
    "    unique_emails_catalog = catalog_df.select(\"email\").distinct().count()\n",
    "    total_records_catalog = catalog_df.count()\n",
    "    print(f\"Unique emails in catalog: {unique_emails_catalog}\")\n",
    "    print(f\"Total records in catalog: {total_records_catalog}\")\n",
    "    print(f\"Duplicate email rate: {(total_records_catalog - unique_emails_catalog) / total_records_catalog * 100:.1f}%\")\n",
    "except:\n",
    "    print(\"\\n‚ö†Ô∏è  Catalog table not available for verification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49c9e5cf-e378-4ffc-9a55-2d05f21f3461",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "List Created Files for Reference"
    }
   },
   "outputs": [],
   "source": [
    "# List all created CSV files and catalog table information\n",
    "print(\"üìÅ Created CSV files and catalog table ready for Auto Loader processing:\\n\")\n",
    "\n",
    "# Show catalog table information first\n",
    "print(f\"üìä Catalog Table: {full_table_name}\")\n",
    "try:\n",
    "    catalog_df = spark.table(full_table_name)\n",
    "    total_size_estimate = catalog_df.count() * 100  # Rough estimate\n",
    "    print(f\"   ‚úÖ Successfully stored in catalog\")\n",
    "    print(f\"   Records: {catalog_df.count():,}\")\n",
    "    print(f\"   Files represented: {catalog_df.select('file_id').distinct().count()}\")\n",
    "    print(f\"   Columns: {', '.join(catalog_df.columns)}\")\n",
    "    print(f\"   Estimated size: ~{total_size_estimate:,} bytes\")\n",
    "    print(f\"   Table location: {full_table_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Catalog table not accessible: {e}\")\n",
    "\n",
    "print(f\"\\nüìÅ Individual CSV Files:\")\n",
    "import glob\n",
    "csv_files = sorted(glob.glob(f\"{csv_dir}/*.csv\"))\n",
    "\n",
    "total_size = 0\n",
    "total_records = 0\n",
    "\n",
    "for i, filepath in enumerate(csv_files, 1):\n",
    "    file_size = os.path.getsize(filepath)\n",
    "    df = pd.read_csv(filepath)\n",
    "    total_size += file_size\n",
    "    total_records += len(df)\n",
    "    \n",
    "    print(f\"{i:2d}. {filepath}\")\n",
    "    print(f\"    Size: {file_size:,} bytes\")\n",
    "    print(f\"    Records: {len(df):,}\")\n",
    "    print(f\"    Columns: {', '.join(df.columns)}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Summary:\")\n",
    "print(f\"   Total CSV files: {len(csv_files)}\")\n",
    "print(f\"   Total records: {total_records:,}\")\n",
    "print(f\"   Total size: {total_size:,} bytes ({total_size/1024:.1f} KB)\")\n",
    "print(f\"   Average records per file: {total_records/len(csv_files):.0f}\")\n",
    "print(f\"   Average file size: {total_size/len(csv_files):.0f} bytes\")\n",
    "\n",
    "print(f\"\\nüí° Next steps for Auto Loader:\")\n",
    "print(f\"1. Option A - Use Catalog Table:\")\n",
    "print(f\"   - Query data directly from: {full_table_name}\")\n",
    "print(f\"   - Filter by file_id or file_name for specific files\")\n",
    "print(f\"   - Use for batch processing or analysis\")\n",
    "print(f\"\")\n",
    "print(f\"2. Option B - Use CSV Files in Volume:\")\n",
    "print(f\"   - Copy CSV files to your Volume paths:\")\n",
    "print(f\"     * For ADD operations: /Volumes/{{catalog}}/{{schema}}/{{volume}}/folder*/add/\")\n",
    "print(f\"     * For REMOVE operations: /Volumes/{{catalog}}/{{schema}}/{{volume}}/folder*/remove/\")\n",
    "print(f\"   - Run Auto Loader to process streaming files\")\n",
    "print(f\"\")\n",
    "print(f\"3. Data Distribution Recommendation:\")\n",
    "print(f\"   - Files 01-05: Use for ADD operations (150 records total)\")\n",
    "print(f\"   - Files 06-10: Use for REMOVE operations (150 records total)\")\n",
    "print(f\"   - This ensures balanced testing of both ADD and REMOVE streams\")\n",
    "\n",
    "# Show the expected directory structure\n",
    "print(f\"\\nüìÇ Recommended Volume directory structure:\")\n",
    "print(f\"/Volumes/{{catalog}}/{{schema}}/{{volume}}/\")\n",
    "print(f\"‚îú‚îÄ‚îÄ batch1/\")\n",
    "print(f\"‚îÇ   ‚îî‚îÄ‚îÄ add/\")\n",
    "for i in range(1, 6):\n",
    "    print(f\"‚îÇ       ‚îú‚îÄ‚îÄ people_data_{i:02d}.csv\")\n",
    "print(f\"‚îú‚îÄ‚îÄ batch2/\")\n",
    "print(f\"‚îÇ   ‚îî‚îÄ‚îÄ remove/\")\n",
    "for i in range(6, 11):\n",
    "    print(f\"‚îÇ       {'‚îî' if i == 10 else '‚îú'}‚îÄ‚îÄ people_data_{i:02d}.csv\")\n",
    "print(f\"‚îî‚îÄ‚îÄ checkpoints/\")\n",
    "print(f\"    ‚îú‚îÄ‚îÄ add_stream/\")\n",
    "print(f\"    ‚îî‚îÄ‚îÄ remove_stream/\")\n",
    "\n",
    "print(f\"\\nüîç Quick verification query for catalog table:\")\n",
    "print(f\"SELECT file_name, COUNT(*) as record_count\")\n",
    "print(f\"FROM {full_table_name}\")\n",
    "print(f\"GROUP BY file_name, file_id\")\n",
    "print(f\"ORDER BY file_id;\")\n",
    "\n",
    "# Execute the verification query if possible\n",
    "try:\n",
    "    print(f\"\\nüìã Executing verification query:\")\n",
    "    verification_df = spark.sql(f\"\"\"\n",
    "        SELECT file_name, file_id, COUNT(*) as record_count\n",
    "        FROM {full_table_name}\n",
    "        GROUP BY file_name, file_id\n",
    "        ORDER BY file_id\n",
    "    \"\"\")\n",
    "    verification_df.display()\n",
    "except:\n",
    "    print(f\"\\n‚ö†Ô∏è  Could not execute verification query on catalog table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6b5011b-7b67-4dc6-9e3e-dd784a5f5c4c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate mock list of lists to segments data"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "\n",
    "result = [f\"#{letter}{str(digit1)}{str(digit2)}\" for letter in string.ascii_uppercase for digit1 in range(10) for digit2 in range(10)]\n",
    "\n",
    "def generate_list_to_segment_table(list_of_list_keys):\n",
    "  buckets = {'blue': [], 'red': [], 'green': []}\n",
    "  for coso in list_of_list_keys:\n",
    "    bucket = random.choice(['blue', 'red', 'green'])\n",
    "    buckets[bucket].append(coso)\n",
    "  for color, items in buckets.items():\n",
    "    print(f\"{color}: {items}\")\n",
    "  # Create DataFrame from buckets\n",
    "  data = [(color, item) for color, items in buckets.items() for item in items]\n",
    "  df = spark.createDataFrame(data, schema=[\"bucket\", \"key\"])\n",
    "  return df\n",
    "\n",
    "keys = random.sample(result, 20)\n",
    "df = generate_list_to_segment_table(keys)\n",
    "\n",
    "display(df)\n",
    "df.write.mode('overwrite').saveAsTable(\"agustin_training_catalog.dev_agustin_panizza_sfmc.segments_keys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2963ab12-48c0-4f13-9516-1515c89cabfe",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760712938778}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Generate historic data"
    }
   },
   "outputs": [],
   "source": [
    "# --- Generate 300 unique new people and save to CSV ---\n",
    "unique_new_people = []\n",
    "unique_emails = set()\n",
    "while len(unique_new_people) < 300:\n",
    "    person = generate_person()\n",
    "    # Ensure email is unique and not in people_pool\n",
    "    if person['email'] not in unique_emails and all(p['email'] != person['email'] for p in people_pool):\n",
    "        unique_new_people.append(person)\n",
    "        unique_emails.add(person['email'])\n",
    "\n",
    "new_people_df = pd.DataFrame(unique_new_people)\n",
    "\n",
    "# Add a 15-character random id to the DataFrame using Python\n",
    "import string\n",
    "import random\n",
    "\n",
    "def random_string(length=15):\n",
    "    chars = string.ascii_letters + string.digits\n",
    "    return ''.join(random.choices(chars, k=length))\n",
    "\n",
    "new_people_df['random_id'] = [random_string(15) for _ in range(len(new_people_df))]\n",
    "\n",
    "new_people_df['list_keys'] = [''.join(random.sample(keys, k=random.randint(0, 3))) for _ in range(len(new_people_df))]\n",
    "\n",
    "display(new_people_df)\n",
    "\n",
    "\n",
    "catalog = \"agustin_training_catalog\"\n",
    "schema = \"dev_agustin_panizza_sfmc\"\n",
    "new_csv_filename = f\"/Volumes/{catalog}/{schema}/mock_files/people_data_historic.csv\"\n",
    "\n",
    "# Create directory for temporary CSV files\n",
    "\n",
    "new_people_df.to_csv(new_csv_filename, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Created new file with 300 unique people: {new_csv_filename}\")\n",
    "print(f\"Shape: {new_people_df.shape}\")\n",
    "print(\"\\nSample records:\")\n",
    "print(new_people_df.head())\n",
    "print(\"\\nData Quality Summary:\")\n",
    "print(new_people_df.isnull().sum())"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8513883491498066,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Create mock files",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
