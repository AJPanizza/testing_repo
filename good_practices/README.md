# good_practices

The 'good_practices' asset bundle was generated by using the Default Python template.

## Getting Started

To deploy and manage this asset bundle, follow these steps:

### 1. Deployment

- Click the **deployment rocket** ðŸš€ in the left sidebar to open the **Deployments** panel, then click **Deploy**.

### 2. Running Jobs & Pipelines

- To run a deployed job or pipeline, hover over the resource in the **Deployments** panel and click the **Run** button.

### 3. Managing Resources

- Use the **Add** dropdown to add resources to the asset bundle.
- Click **Schedule** on a notebook within the asset bundle to create a **job definition** that schedules the notebook.

## Documentation

- For information on using **Databricks Asset Bundles in the workspace**, see: [Databricks Asset Bundles in the workspace](https://docs.databricks.com/aws/en/dev-tools/bundles/workspace-bundles)
- For details on the **Databricks Asset Bundles format** used in this asset bundle, see: [Databricks Asset Bundles Configuration reference](https://docs.databricks.com/aws/en/dev-tools/bundles/reference)

## Testing

Run tests against Databricks (default):

```bash
uv run pytest --compute=databricks
```

Run tests locally (requires PySpark + Delta support installed):

```bash
uv run pytest --compute=local
```

You can also set `TEST_COMPUTE=databricks|local` to control the default mode.

### Delta tables in local tests

Local-mode tests use a local Spark session configured with Delta Lake extensions so you can create/write/read Delta tables during tests (via `delta-spark`).

For more detail (compute selection, markers, fixture loading, and Delta setup), see `good_practices/testing.md`.
